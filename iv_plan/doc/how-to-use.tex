\documentclass[11pt]{jreport}
\setlength{\topmargin}{-15mm}
\setlength{\evensidemargin}{-2mm}
\setlength{\oddsidemargin}{3mm}
\setlength{\textheight}{24.5cm}
\setlength{\textwidth}{15cm}
%%
\usepackage[dvipdfm]{graphicx}
\usepackage{enumerate}

\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm} % 数式の中のBold (\bm{})
\usepackage{amsmath} % 数式の中の改行 (\begin{gather} \\ )

\usepackage{listings,jlisting}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand {\figref}[1] {図\ref{#1}}
\newcommand{\tabref}[1] {表\ref{#1}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\renewcommand{\subfigtopskip}{1pt}
\renewcommand{\subfigbottomskip}{1pt}
\renewcommand{\subfigcapskip}{1pt}
\setlength{\floatsep}{6pt}           % 図表と図表の間のマージン
\setlength{\dblfloatsep}{6pt}        % ↑の二段組 version
\setlength{\textfloatsep}{6pt}       % 図表と本文の間のマージン
\setlength{\abovecaptionskip}{-2pt}   % 図表の caption と図表本体の間のマージン
\setlength{\belowcaptionskip}{2pt}   % 図表の caption 下部のマージン

\input amssym.def

\author{東京大学　情報理工学系研究科　稲葉研究室}
\title{手先カメラを用いた双腕ロボットによる\\
マニピュレーションシステム\\
操作手順書}
%\date{2010/4/10}

\begin{document}
\setlength{\baselineskip}{1.5zw}

\maketitle

\tableofcontents


\chapter{システム概要}

本サービスは，工場での部品整理をイメージしたものである．
具体的には手先のカメラを用いて作業台上の部品を認識し,
両手で箱に整理して入れる機能を実現する．
手を動かすことで複数の対象物を認識，両手の干渉を考慮して
同時にアプローチできる対象物を選択する．

 \section{全体のモジュール構成}

 以下に，本システムで利用するモジュールの一覧を示す．
\begin{itemize}
 \item app-recog
 \item CameraComp
 \item LoadPictureComp
 \item iv\_plan\_hironx
 \item HiroNXInterface
\end{itemize}
ファイルシステム上の場所は必ずしも重要ではないが，
ディレクトリ構成は揃えておくと
本ドキュメントとあわせて理解しやすい．

\begin{figure}[htb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/system.png}
  \caption{サービスイメージ}
  \label{fig:system}
 \end{center}
\end{figure}
\begin{figure}[htb]
 \begin{center}
  \includegraphics[width=1.0\linewidth]{figure/rtc_diagram.png}
  \caption{全体のモジュール構成}
  \label{fig:rtc_diagram}
 \end{center}
\end{figure}


\section{認識部}

\subsection{エッジベース二次元対象物認識モジュール(AppRecog)}

http://openrtm.org/openrtm/ja/project/NEDO\_Intelligent\_PRJ\_HiroAccPrj\_5002

HiroNXの手先に取り付けられたUSBカメラで対象物を認識するためのモジュール
である．カメラパラメータはデータポートを通して画像と一緒に送られてくるも
のを利用する．正しいカメラパラメータが入っていなくても対象物の認識はでき
るが，位置および姿勢は正しく推定されない．

\subsubsection{ダウンロードとコンパイル}

https://code.google.com/p/app-recog/
からダウンロードします．
\begin{lstlisting}
 $ tar xvfz AppRecog-0.1.0.tgz
 $ cd app-recog
 $ make
\end{lstlisting}

\subsubsection{開発・動作環境}

\begin{itemize}
 \item Ubuntu Linux 10.04 LTS
 \item OpenRTM-aist 1.0.0-RELEASE C++版
 \item OpenCV 2.3
\end{itemize}
 OpenCVのバージョンに注意してください．

\subsubsection{インタフェース}

\begin{itemize}
 \item データポート
       \begin{itemize}
	\item 入力: Img::TimedCameraImage (Img.idl) \\
	      画像出力共通インタフェース準拠のカメラモジュールから，
	      画像及び，カメラパラメータを受取ります．
	\item 出力: TimedRecognitionResult (Vision.idl) \\
	      認識結果共通インタフェースにしたがい，対象物体の位置姿勢を出力します．
	      Img::TimedCameraImage 処理結果を画像として出力します．
       \end{itemize}
 \item サービスポート \\
       認識対象のモデルを設定するために使います．
       あらかじめ．ModelFiles/ModelList.txtにモデルIDとモデル定義ファイ
       ル名を記述し，モデルIDを引数としてサービスコールを行います．
       setModelID(i)は，i番のモデルを使用することを意味します．
\end{itemize}
 認識結果はTimedRecognitionResultによって出力されます．
 具体的な出力内容は以下の通りです．現在，対象物の姿勢以外は入っていま
 せん．
\begin{verbatim}
  0: 0, 1: 0, 2: 0, 3: 0, 4: 0
  5: 0, 6: 0, 7: 0, 
  8: R00,  9: R01, 10: R02, 11: Tx
 12: R10, 13: R11, 14: R12, 15: Ty
 16: R20, 17: R21, 18: R22, 19: Tz
\end{verbatim}

\subsubsection{カスタマイズ}

連続的に送られてくる画像に対して認識を行いますが，認識結果の時間方向の連続性
は考慮せず，各フレームで一番尤度が高い位置を計算し，その尤度が閾値以上で
あれば検出結果を返します．

% 認識手法の簡単な説明
% 閾値の意味について
% サービスによる認識対象の切り替え
% 認識モジュールにおける対象物モデルの定義の仕方

モデルと実画像のマッチングは，画像上で行われます．検出した位置，姿
勢，スケールからカメラパラメータを用いて，カメラ座標系における対象物の位
置，姿勢が計算されます．
画像座標での$(x,y,\theta)$の探索範囲，検出の閾値は，
コンフィグファイルAppRecog.confで指定できます．
また，モデル定義ファイルは./ModelFileの中に置き，ファイルは頂点と辺によっ
て構成されています．

\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/apprecog.png}
  \caption{認識例}
  \label{fig:apprecog}
 \end{center}
\end{figure}

\subsection{カメラ共通I/F準拠の画像キャプチャモジュール(CameraComp)}

http://www-arailab.sys.es.osaka-u.ac.jp/CameraIF/

大阪大学により開発され画像キャプチャモジュールCameraCompをダウンロード，
コンパイルする．ログ画像によるテストを行うため，LoadPictureCompモジュー
ルも同様にダウンロードするとよい．


\subsection{認識部の動作確認}

実際にカメラモジュールと接続し，オンラインでテストを行う．
このときのモジュール接続は\figref{fig:test_by_camera}のようになり，
実行手順は，以下の通りである．
\begin{enumerate}
 \item 認識モジュールAppRecogとキャプチャモジュールをそれぞれ実行する．
 \item rtshellで画像の入出力を接続する(system editor上で操作してもよい)．
 \item 2つのRTCをactivateする．
\end{enumerate}

\begin{lstlisting}
 $ cd CameraComp
 $ ./CaptureCameraComp
 別端末で
 $ cd app-recog/
 $ build/bin/AppRecogComp
 別端末で（rtctreeでのパスは適当に補完する）
 $ rtcon CaptureCamera0.rtc:CameraImage AppRecog0.rtc:InputImage
 $ rtact CaptureCamera0.rtc AppRecog0.rtc
\end{lstlisting}

初期設定で認識範囲のスケールを絞ってあるため，認識できない場合は
対象物までの距離をいろいろ変えてみる．
また，背景に模様がなく，対象物と異なる色のものを置くと認識しやすくなる．

テストとして，カメラモジュールをLoadPictureCompモジュールに差し替え，
あらかじめ撮っておいた画像を用いてテストを行う場合，
CaptureCameraCompをLoadPictureCompに置き換えた接続となる．
ログ画像の指定は，LoadPictureCompモジュールのLoadPicture.confで
行う．AppRecogモジュール付属の画像data/parts4.jpgを用いて確認を行うとよ
い．
LoadPicture.confで読み込む画像を指定するには，rtc.confに
\begin{verbatim}
Processing Module.LoadPicture.config_file: LoadPicture.conf
\end{verbatim}
を，LoadPicture.confに
\begin{verbatim}
conf.default.string_file_name: parts4.jpg
\end{verbatim}
と記述する．parts4.jpgは読込みたいファイルの名前を書く．

% \begin{figure}[tb]
%  \begin{center}
%   %\includegraphics[width=0.5\linewidth]{figure/recog_test_log.png}
%   \caption{ログ画像を用いたテスト}
%   \label{fig:test_by_image}
%  \end{center}
% \end{figure}

\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/recog_test_cam.png}
  \caption{USBカメラを用いたテスト}
  \label{fig:test_by_camera}
 \end{center}
\end{figure}


\section{動作生成部}

\subsection{(VPython版)HiroNX動作生成モジュール}

http://openrtm.org/openrtm/ja/project/NEDO\_Intelligent\_PRJ\_HiroAccPrj\_5003

Python対話環境において、幾何モデルを用いた動作生成システムを柔軟に構築するための
スクリプト群です．RtcHandleを用いて対話環境からRTC構成によるシステムの
各モジュールと通信を行うことで，システムの統合を行います．
RRT-connectによる双腕の干渉を考慮した動作計画機能を提供し，人手に
よる動作記述とプランナによる動作生成をスムーズに統合できます．
また，RTCとして，作業共通インタフェースを実装する動作生成モジュールとし
て利用することもできます．

\subsubsection{ダウンロードとコンパイル}


\begin{lstlisting};
 $ cd iv-plan-hironx/iv_plan/scripts; ./install-debs.sh
 $ cd iv-plan-hironx/iv_plan/; make
 環境変数PYTHONPATHに iv-plan-hironx/iv_plan/srcを追加
\end{lstlisting}

環境によってVPythonが正しく動作しない場合があるため，また，debパッケージ
でインストールされるバージョンではサポートされていない機能を利用するため，
VPythonはUbuntu 10.04LTCの標準パッケージより新しいものにパッチを当て，コ
ンパイル済みのものを利用します．これは，make時にダウンロードします．
したがって，標準のpython-visualパッケージ
をインストールしている場合は，それがpython上で先にロードされることがない
ように注意する必要があります．
ikfastにより生成されたHIRO-NX用逆運動学計算のソースは同梱されており，PQP
のソースはmake時にダウンロードし，それぞれコンパイルされます．
それ以外に必要なものはaptコマンドでインストールします(install-deb.sh).

\subsubsection{開発・動作環境}

\begin{itemize}
 \item Ubuntu Linux 10.04 LTS 32bit/64bit
 \item OpenRTM-aist 1.0.0 (Python版)
\end{itemize}

\subsubsection{基本的な使い方}

まず，タスク記述，動作計画，認識および制御モジュールとの接続を
１つのPythonシェル上で行う方法を説明します．

\begin{lstlisting}[label=src:branch]
 $ cd iv_scenario/src
 $ ipython test.py
 >>> test1()

 立体パズルデモ
 $ ipython puzzle.py
 >>> demo() # パズル配置を元に戻すのは reset_puzzle()

 シミュレーションによる部品の箱詰め
 $ ipython demo_wexpo.py
 >>> demo(False)
\end{lstlisting}

ウィンドウ操作は，右ドラッグで回転，中ドラッグでズームです．
プログラムの終了はCtrl+dでインタープリタを抜けた後，GUIの閉じるボタンを
押します．Ctrl+cはトラップされています．GUIの閉じるボタンを押すのが面倒
な場合はCtrl+\textbackslash でSIGQUITを送ることで終了させることができます．

Pythonシェル上でロボットの姿勢を変更したり，環境中の物体を移動させたり，
動作計画を行う場合の操作に関しては付録を参照してください．

外部RTCとの通信にはRtcHandleを利用しています．

(http://staff.aist.go.jp/t.suehiro/rtm/rtc\_handle.html)

% \begin{itemize}
%  \item 基本的に，ロボットクラス，ロボットインスタンスごとのカスタマイズは既存クラスを拡張することで行います．
%  \item robot.pyからhironx.py
%  \item hironx\_params.py
%        個体差がある各種transformと，利用する外部モジュール定義
%  \item 環境モデルの定義 \\
%        (pythonのデータ構造で与える．scene\_object.py)
%  \item 他の機能として，VRMLのローダ，センサ取り付け位置の推定機能があり
%        ます.
% \end{itemize}


\begin{figure}[tb]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/planner_scripts.png}
  \caption{動作生成モジュール}
  \label{fig:planner_scripts}
 \end{center}
\end{figure}

% \subsubsection{インタフェース}


\subsection{HiroNXInterface}

http://www.openrtm.org/openrtm/en/node/4645

双腕ロボットの制御コマンドの共通インタフェースに準拠する
HiroNX用制御モジュールです．
詳細については，開発元である産業技術総合研究所のドキュメントを参照してく
ださい．

\subsection{動作生成部の動作確認(対話環境における動作生成)}

 % $ 起動スクリプト
 % IP/ホスト名の違いはどこを直せばよいか？

ここからは，HiroNX実機を使います．
まず，HiroNXInterface制御モジュールを起動，activate，接続し，利用可能な
状態にします（詳細はHiroNXInterfaceのドキュメントを参照してください）．
HiroNXInterfaceモジュールは起動後GUI上の「RTC Status」が緑になった状態で
「Set up Robot」ボタンを押しRTCまわりの初期化を行う必要がある点に注意し
てください．
次に，HiroNX両手の手先カメラのキャプチャ及び，AppRecog認識モジュールを起
動します．左右で同じ名前のコンポーネント群を起動するため，別々のRTC名前
空間に起動します．
\begin{lstlisting}[label=src:branch]
 $ cd iv_plan/scripts
 $ ./run.sh 
 このスクリプトはROSのツールであるrospack findを使います
 もし，コンポーネントが上手く接続およびactivateされない場合
 は，./comcon.shおよび./comact.shを再度実行してください．
\end{lstlisting}
カメラが認識できていない可能性がある場合は，xawtvコマンドなどでカメラ自
体が認識されているか確認してください．

認識関係のモジュールを登録するネームサーバはscriptsディレクトリの
rtc\_left.confおよびrtc\_right.confで指定します．
また，動作生成側からこれらのモジュールにアクセスする必要があり，
検索先のネームサーバはPythonプログラムを起動するディレクトリにある
rtc.confから取得します．以下のようにiv\_scenarioディレクトリで
pythonプログラムを起動する場合，iv\_scenario/rtc.confが読み込まれます．

次に，以下のコマンドでデモプログラムを起動し，認識器と通信し認識結果を取得でき
ること，HiroNX本体のモジュールと通信し，関節角の取得および，動作コマンド
の送信を行なうことができることを確認します．
\begin{lstlisting}[label=src:branch]
 $ cd iv_scenario/src

 外部モジュールとの通信
 $ ipython interface_wexpo.py
 >>> rr.get_joint_angles() # 実機の関節角度列を取得
 >>> r.prepare() # シミュレータ内で姿勢を変更
 >>> sync() # 実機の姿勢をシミュレータにあわせる(実機が動くので注意)

 >>> rr = MyHiroNxSystem(portdefs) # 外部モジュールとの通信インタフェー
 スオブジェクトを作成

 認識できる位置に対象物を置いて
 >>> rr.detect(sensor='rhandcam') # 右手ハンドカメラでの認識
 >>> rr.detect(sensor='lhandcam') # 左手ハンドカメラでの認識
 (対象物を認識できるまでブロックします)
\end{lstlisting}

 認識結果の世界座標への変換
\begin{lstlisting}[label=src:branch]
 $ ipython demo_wexpo.py # 内部で上のinterface_wexpo.pyを利用しています．
 >>> detect(sensor='rhandcam')
 >>> detect(sensor='lhandcam')

 >>> f = detect(sensor='rhandcam')
 >>> show_frame(f) # 認識結果の表示
 # シミュレータ内で認識位置に箱A0を移動させる
 >>> env.get_object('A0').locate(f) 

 # 作業台上で両手を動かし，対象物を検出する
 # 上手く検出できれば，シミュレータ内の箱4つが正しい位置に移動する
 >>> look_for()
\end{lstlisting}

% \subsection{動作生成を動作計画モジュールとして分離する場合}

% シミュレーション環境
% 外部モジュールとの通信
% 動作コマンド送信，状態読み込み


\chapter{準備}

必要に応じて行うロボットごとに以下の作業を行う．

\section{モデルファイルの修正（力センサありとなし）}


\section{キャリブレーション}

ロボットの個体差を修正する作業である．
デフォルト値はHiroNX14号機のものであり，
精度を出すには各機体ごとに行う必要がある．

\subsection{カメラキャリブレーション}

RT-middlewareのコンポーネントでもROSのノードでも何でもよいので
単眼カメラのキャリブレーションを行い，CameraCompが読み込めるように
する．本システムにおいて，run.shスクリプトでRTCを起動する場合，
scriptsディレクトリにあるcamera\_left.ymlおよびcamera\_right.ymlにそれぞ
れのカメラの内部パラメータを記述する．
その後，エッジベース二次元対象物認識モジュールにおいて，対象物の位置，距
離が正しく出力されているかどうか確認しておくとよい．

\subsection{カメラ取り付け位置のキャリブレーション}

現状では，チェッカーボードの認識にROSのノードを使用している．
キャリブレーションプログラムは，学習データとしてロボットの各姿勢における
チェッカーボードの姿勢を入力とします．チェッカーボードの姿勢はtfとして
publishされたメッセージをpythonプログラムで受信します．
しがって，準備として，
\begin{itemize}
 \item ROSのカメラノードにより画像および上記カメラパラメータがトピックと
       してpublish
 \item checkerboard detectionノードでそれらをsubscribeし，推定したtfがpublish
\end{itemize}
される状態にしておく必要がある．

\subsubsection{手順}

\begin{lstlisting}
 1, 机の上にチェッカーボードを置く

 2, 首を動かして学習データをとる
 $ ipython hironx_calib.py
 >>> res = record_data()  
 # 手先を動かして画像とそのときの関節角度値を取得する.
 # すべての姿勢でチェッカーボードが視野に入り，
 # 安定して認識できているかどうかを確認する．

 3, 頭リンクからkinectカメラへのtransformを計算する
 >>> f = calibrate(res, height=960.0)

 4, transformの書き換え
 >>> r.Thd_kinectrgb = f

 5, 確認（正しい位置でキャリブボードのフレームが止まっていれば成功）
 >>> play_data(res)

 6, デフォルト値の更新
 hironx_params.pyのThd_kinectdepthを書き換える
\end{lstlisting}

\begin{itemize}
 \item 上のコマンドは頭部kinectの場合なので，ハンドカメラの場合に変更す
       る
 \item 原理について
\end{itemize}

\chapter{デモの実行}

\section{認識部のモジュール起動と接続}

\begin{lstlisting}
 $ cd iv_plan/scripts
 $ ./run.sh -c
\end{lstlisting}
これにより，両手に対応するカメラ，認識モジュール，HiroNXInterface制御モ
ジュールが起動，接続され，さらに各モジュールがactivateされます．
HiroNXInterfaceモジュールは起動後GUI上の「RTC Status」が緑になった状態で
「Set up Robot」ボタンを押しRTCまわりの初期化を行う必要がある点に注意し
てください．

ここで，rtc.conf,左右カメラの左右のカメラが逆になっていないかどうか確認
してください．逆になっている場合は，CaptureCameraLeft.confと
CaptureCameraRight.confに記述してあるconf.default.int\_camera\_idの番号を
逆にして，run.shを再実行してください．

\section{動作生成プログラムの起動(1つのpythonプロセス上での実行)}

\begin{itemize}
 \item テーブル上に対象物を4つ配置し，
       look\_for()関数を実行します．
       手先を動かして机上の対象物を認識してみます．
 \item このとき，画像中で対象物が正しく認識されているか，認識結果がシミュ
       レータ中に正しく表示されているかどうかを確認します．
 \item これで準備が整ったので，実際にデモを実行します．
       実行後，ロボットはテーブルをスキャンし，対象物を認識
       demo()
\end{itemize}


\appendix
\chapter{VPython環境でのプログラミング}

フレームやロボット，環境中の物体操作等をpython上で行う方法について説明し
ます．


\begin{verbatim}

# 1, pythonを使う

dir(r) # 関数やメソッドの一覧
r. [TAB] # メソッド名等の補間（ipythonの機能）
help(r) # 引数や説明

# コマンドライン
# Ctrl+c
# Ctrl+p / Ctrl+n
# Ctrl+r


# 2, 環境中の物体操作

env
env.get_objects() # 環境中の物体一覧
[x.name for x in env.get_objects()] # 物体名一覧

# 物体は名前で管理されている。
# 同じ名前の物体は追加できないので、一度削除するか、名前を変えて追加する。
putbox() # テーブル上に箱を置く
b = env.get_object('box') # 名前で物体取得
put_box(name='box2') # 名前を変えると違う物体
env.delete_object('box2')

frm = b.where() # 物体の位置と姿勢を取得

# ロボットの移動、回転 ( world=>basejointの座標変換の変更 )
r.go_pos(-150,500,0) # 2D座標, (x,y,theta)
r.go_pos(-150,0,pi/2) # 横を向く


# 3, 自作サンプルの書き方
emacs mysample.py

== mysample.py ==
from demo import *

... 適当なコード ...
== ==

ipython mysample.py

# サンプルを修正後は、以下を実行すると
# mysample.pyの修正が反映される

import mysample # 最初の一回だけ

reload(mysample)
from mysample import *


# 4, ロボットの姿勢の操作

# joint, linkの取得
r
r.get_joints()
r.get_links()

# ジョイント名の取得
j0 = r.get_joints()[0]
dir(j0)
j0.angle
j0.name
map(lambda x: x.name, r.get_joints())
[x.name for x in r.get_joints()]

# 関節角度の表示
r.get_joint_angles()
# これは下のコードと同じ
[x.angle for x in r.get_joi

# 腕だけ
r.get_arm_joint_angles()
r.get_arm_joint_angles(arm='left')
r.set_arm_joint_angles(angles)
r.set_arm_joint_angles(angles, arm='left')

# ハンドだけ
r.get_hand_joint_angles()
r.get_hand_joint_angles(hand='left')
r.set_hand_joint_angles(angles)
r.set_hand_joint_angles(angles, hand='left')

# 関節１つを変える
r.get_joint_angle(0) # ID
r.set_joint_angle(0, 0.5) # IDと角度[rad]

# 初期姿勢に戻す
r.reset_pose()

# あらかじめいくつかの姿勢が定義されている
r.poses
r.poses.keys()

# r.reset_pose()は以下と同じ
r.set_joint_angles(r.poses['init'])

# 手のポーズ
r.hand_poses
r.hand_poses['open']
r.set_hand_joint_angles(r.hand_poses['open']) # 手を開く
r.set_hand_joint_angles(r.hand_poses['close']) # 手を閉じる

# アニメーション（首を振ってみる）
arange(0, 1, 0.2)
for th in arange(0,1,0.2):
        r.set_joint_angle(1, th)
        time.sleep(0.5)

# ちなみに左右の腕の関節書く取得は以下のコードと同じ
r.get_joint_angles()[3:9]
r.get_joint_angles()[15:19]



# 5, 座標系(frame)について
# 3x3回転行列と3次元ベクトル = 4x4の同次数行列
# euler角, 自由軸回転, quaternion

help(VECTOR)
help(MATRIX)
help(FRAME)

# 値の生成(constructor)
VECTOR()
MATRIX()
FRAME()
v=VECTOR(vec=[100,0,0])
MATRIX(angle=pi/2, axis=VECTOR(vec=[0,0,1]))
MATRIX(c=pi/2) # a,b,cを同時に指定できないので注意
m=MATRIX([[1,0,0],[0,1,0],[0,0,1]])
FRAME()

frm.mat # 姿勢部分
frm.vec # 位置部分

# 演算
v*v # ベクトル積
dot(v,v) # 内積
v+v # 和
2*v # スカラー倍
# 行列は姿勢表現専用（直行行列）
m*m # 積
-m # 逆行列
# -mはじ実装は転置行列
# スカラー倍、行列和は定義されない(配列の結合解釈される)

# 姿勢表現間の変換
m.abc() # 行列=>euler
m.rot_axis() # 行列=>自由軸回転
# euler=>回転行列、自由軸回転=>回転行列は上記のMATRIXのコンストラクタ

# 位置と姿勢を合わせた表現（同時行列）
FRAME(mat=m, vec=v)
FRAME(xyzabc=[x,y,z,a,b,c])

f=FRAME(vec=[500,0,1000])
show_frame(f)
f.mat = MATRIX(a=pi/4)
show_frame(f)

f.mat = MATRIX(a=pi/4)*MATRIX(b=pi/4)
show_frame(f)

# 座標系の親子構造
FRAME.affix() # 座標系の親子関係の定義
FRAME.unfix() # 座標系の親子関係の削除
FRAME.set_trans() # 親子間の座標変換の設定
f.rel_trans # 親子間の座標変換の取得

# 物体追加
# putbox()の記述を参照
# 表示用形状とFRAMEを作り、適当な親座標系の子FRAMEとして、座標系ツリーに挿入する

env.insert_object('box2') # 物体追加
env.delete_object('box2') # 物体削除(子フレームの物体も削除される)


# 6, 逆運動学(Inverse Kinematics, IK)の利用

# 手首位置，姿勢の取得
r.fk() # 順運動学計算，現在の手先FRAMEを返す．デフォルトは右手．
r.fk(arm='left') # 左手は明示的に引数で指定する

# 目的位置へのアプローチ

putbox() # 手が届きそうなところに置く
objfrm = detect()

# アプローチ姿勢，把持姿勢の計算（2つずつ求まる）
afrms, gfrms = pl.grasp_plan(objfrm)

# アプローチ姿勢を表示
# (FRAMEからCoordinateObjectを作って、環境にinsert_objectする)
show_frame(afrms[0])
show_frame(afrms[1])

# 目標位置へ手を伸ばすための関節角度を計算する
r.ik(afrms)
help(r.ik)
# - IKは目標手先FRAMEの集合を引数にとり、
#   初期姿勢から関節空間での重み付き距離順に並べた解の列を返す
#   目標手先フレーム１つを引数に指定してもよい
# - 解が存在しなければNone
# - ほとんどの場合、最初の解を採用すればよい

asols = r.ik(afrms)
r.set_arm_joint_angles(asols[0]) # 腕の角度のみ

gsols = r.ik(gfrms)
r.set_arm_joint_angles(gsols[0]) # 腕の角度のみ

# 腰を使う
# 腰yaw軸を使うと手の届く範囲が大きく広がる

asols = r.ik(afrms, use_waist=True) # 返値の形式が違うので注意 (waist_yaw, arm_angles)
th,js = asols[0]
r.set_joint_angle(0,th) # 腰を回す
r.set_arm_joint_angles(js) # 腕の姿勢を変える

# 他の解も表示してみる
for th,js in asols:
        r.set_joint_angle(0,th)
        r.set_arm_joint_angles(js)
        time.sleep(0.5)

# 物体をハンドに固定する
tgtobj = env.get_object('box0')
handjnt = r.get_joint('RARM_JOINT5')
reltf = (-handjnt.where())*tgtobj.where()
tgtobj.unfix()
tgtobj.affix(handjnt, reltf)

# 手先軌道での動作生成 (収束IKではなく、初期姿勢に近い解を明示的に選択している)

# 軌道の作成
traj = CoordinateObjects('trajectory')
traj.append( ... ) # 適当なフレームを追加する
env.insert_object(traj, FRAME(), env.get_world()) # 世界座標相対で軌道を定義
traj.coords

# 削除したいときは
env.delete_object('trajectory')

# 7, 実機を動かす

# 実機とのインタフェース

rr = RealHIRO()
rr.connect() # 認識処理が始まっていなければ開始指令を送る
rr.get_joint_angles() # tf and joint state by ROS
js = r.get_joint_angles()
# socket(+pickle) to jython script
# scequencer, wait
duration = 4.0
rr.send_goal(js, 4.0) # blocking
rr.send_goal(js, 4.0, wait=False) # non-blocking
# blockingな呼出しも最大10[sec]でtimeoutする仕様

# 実際には、モデルで姿勢を確認してから以下を実行するのが便利
sync() # デフォルトは4[sec]で実機をモデルに同期させる
sync(duration=3.0) # 時間を変える

# チェッカーボードの認識
detect() # 認識結果がboxとして表示される
objfrm = detect() # 実機の場合はROSの識別器からtfを受信
                  # simulationの場合は直接、物体位置を読む

# Linkに沿った座標変換
# 認識結果は、カメラ=>対象物の座標変換であるので、
# 世界座標=>対象物の座標変換に直す
# detect()関数の中で行っている処理
r.Thd_leye # 頭リンク => 左目カメラへの変換
Tleye_obj = rr.detect()
Twld_hd = r.get_link('HEAD_JOINT1_Link').where()
Twld_obj = Twld_hd * r.Thd_leye * Tleye_obj


# 実機の場合
hand_cam_demo()

# シミュレータで実行するときは，先に手が届く場所に箱を置いておく
# 'box0' => marker 0, 'box1' => marker 1 に対応
putbox(name='box0', vaxis='y')
putbox(name='box1', vaxis='y')
hand_cam_demo()

# もう一度実行したいとき
env.delete_object('box0')
putbox(name='box0', vaxis='y')


# ハンドカメラはAR-toolkitによるmarker認識(複数物体対応)
# 返り値は (マーカ番号、カメラ=>マーカの座標変換)のリスト
# non-blockingで最新の認識結果を返す
# 過去 thre [sec]以内に認識に成功していないマーカは返さない
# デフォルトは thre = 0.5[sec]

lupus@ roslaunch Sense sense_lhand_ar.launch
rr.detect(camera='lhand') # 左手カメラによる認識

# 平行グリッパの間隔指定把持
r.grasp(width=65, hand='right') # 指の間隔がwidth[mm]になるように指を動かす
r.grasp(65) # これでも同じ

# アプローチ距離の指定
pl.reaching_plan(objfrm, approach_distance=80) # 少し遠くからアプローチ


# 動作計画(仕様が変わる可能性大)
traj = test_plan() # RRT-connectによる動作計画
show_traj(traj) # 軌道の表示
opttraj = pl.optimize_trajectory(traj) # 軌道の最適化
show_traj(opttraj)
opttraj = pl.optimize_trajectory(opttraj) # さらにスムージング

# 干渉チェックにはPQPを利用
# ロボットのモデルVRMLそのまま

# 干渉チェック対象物の追加例
r.add_collision_object(env.get_object('table top'))

\end{verbatim}

% \addcontentsline{toc}{chapter}{参考文献}
% \markboth{参考文献}{参考文献}
% \bibliographystyle{junsrt}
% \bibliography{p2009}


\end{document}
