\documentclass[11pt]{jreport}
\setlength{\topmargin}{-15mm}
\setlength{\evensidemargin}{-2mm}
\setlength{\oddsidemargin}{3mm}
\setlength{\textheight}{24.5cm}
\setlength{\textwidth}{15cm}
%%
\usepackage[dvipdfm]{graphicx}
\usepackage{enumerate}

\usepackage{subfigure}

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bm} % 数式の中のBold (\bm{})
\usepackage{amsmath} % 数式の中の改行 (\begin{gather} \\ )

\usepackage{listings,jlisting}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\newcommand {\figref}[1] {図\ref{#1}}
\newcommand{\tabref}[1] {表\ref{#1}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}

\renewcommand{\subfigtopskip}{1pt}
\renewcommand{\subfigbottomskip}{1pt}
\renewcommand{\subfigcapskip}{1pt}
\setlength{\floatsep}{6pt}           % 図表と図表の間のマージン
\setlength{\dblfloatsep}{6pt}        % ↑の二段組 version
\setlength{\textfloatsep}{6pt}       % 図表と本文の間のマージン
\setlength{\abovecaptionskip}{-2pt}   % 図表の caption と図表本体の間のマージン
\setlength{\belowcaptionskip}{2pt}   % 図表の caption 下部のマージン

\input amssym.def

\author{東京大学　情報理工学系研究科　稲葉研究室}
\title{手先カメラを用いた双腕ロボットによる\\
マニピュレーションシステム\\
操作手順書}
%\date{2010/4/10}

\begin{document}
\setlength{\baselineskip}{1.5zw}

\maketitle

\tableofcontents


\chapter{システム概要}

本サービスは，工場での部品整理をイメージしたものである．
具体的には手先のカメラを用いて作業台上の部品を認識し,
両手で箱に整理して入れる機能を実現します．
手を動かすことで複数の対象物を認識，両手の干渉を考慮して
同時にアプローチできる対象物を選択します．

\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=1.0\linewidth]{figure/system.png}
  \caption{サービスイメージ}
  \label{fig:system}
 \end{center}
\end{figure}

 \section{全体のモジュール構成}

 以下に，本システムで利用するモジュールの一覧を示します．
 これらを入手し，実行可能な環境を整えてください．
\begin{itemize}
 \item app-recog
 \item CameraComp
 \item (LoadPictureComp)
 \item iv\_plan\_hironx
 \item HiroNXInterface
\end{itemize}
ファイルシステム上の場所は必ずしも重要ではないですが，
１つのディレクトリにこれらを置いておくと，本ドキュメントとあわせて理解し
やすいです．これらのモジュールは，1台の計算機上で実行しても，動作生成部
を別の計算機で実行してもどちらでも構いません．
認識部は通信負荷を考えるとカメラがつながっているホスト上で実行することを
推奨します．

\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=1.0\linewidth]{figure/rtc_diagram.png}
  \caption{全体のモジュール構成}
  \label{fig:rtc_diagram}
 \end{center}
\end{figure}


\section{認識部}

\subsection{エッジベース二次元対象物認識モジュール(AppRecog)}

http://openrtm.org/openrtm/ja/project/NEDO\_Intelligent\_PRJ\_HiroAccPrj\_5002

HiroNXの手先に取り付けられたUSBカメラで対象物を認識するために利用します．
両手先のカメラそれぞれでカメラモジュールとともに実行します．
使い方はモジュール付属のドキュメントを参照してください．

\subsection{カメラ共通I/F準拠の画像キャプチャモジュール(CameraComp)}

http://www-arailab.sys.es.osaka-u.ac.jp/CameraIF/

大阪大学により開発され画像キャプチャモジュールCameraCompをダウンロード，
コンパイルします．利用方法は，本モジュールのドキュメントおよび，
上記AppRecog付属のドキュメントを参照してください．

\section{動作生成部}

\subsection{(VPython版)HiroNX動作生成モジュール}

認識結果を世界座標への変換，HiroNXの動作の生成，タスク記述を行なうために
利用します．詳しくは，モジュール付属のドキュメントを参照してください．

\subsection{HiroNXInterface}

http://www.openrtm.org/openrtm/en/node/4645

双腕ロボットの制御コマンドの共通インタフェースに準拠する
HiroNX用制御モジュールです．
詳細については，開発元である産業技術総合研究所のドキュメントを参照してく
ださい．


\chapter{準備}

必要に応じてデモ環境，ロボットごとに以下の作業を行います．

\section{力センサの有無，テーブルの高さ}

手首に力センサがある場合とない場合で，ロードするモデルを変更する必
要があります．サンプルプログラムは，力センサがある場合に手首のリンク長を変更，円筒形状
を挿入したものです．力センサがない場合は，
\verb|HiroNX|インスタンスを生成するときに，オプション\verb|forcesensors=False|を指定します．

また，シミュレータ内シーンはiv\_plan/src/scene\_objects.pyで定義された
Pythonのデータ構造を解釈して生成されます．
デフォルトで高さ700[mm]の東大仕様テーブル(table\_scene())と
高さ735[mm]のAIST仕様テーブル(table\_scene\_aist())が用意されています．
それ以外のシーンを作成したい場合は，scene\_objects.pyを参考に，シーン記述
を作成し，env.load\_scene()で環境にロードしてください．

\section{キャリブレーション}

ロボットの個体差を修正する作業です．カメラの内部パラメータおよび，ロボッ
トのリンクに対するカメラの取り付け位置を計算します．
デフォルト値はHiroNX14号機のものであり，精度を出すには各機体ごとに行う必要があります．

\subsection{カメラのキャリブレーション}

RT-middlewareのコンポーネントでもROSのノードでもよいので単眼カメラのキャ
リブレーションを行い，CameraCompが読み込めるようにします．本システムにお
いて，run.shスクリプトでRTCを起動する場合，scriptsディレクトリにある
camera\_left.ymlおよびcamera\_right.ymlにそれぞれのカメラの内部パラメー
タを記述します．
その後，エッジベース二次元対象物認識モジュールにおいて，対象物の位置，距
離が正しく出力されているかどうか確認しておいてください．
キャリブレーションを行なうときは，以下の関数を実行して手先カメラを正面に
向ける(\figref{fig:camcalib_pose})と作業が容易になります．

\begin{lstlisting}
 $ ipython
 >>> from hironx_calib import *
 >>> handcam_calib_pose()
 >>> sync()
\end{lstlisting}

\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=0.5\linewidth]{figure/camcalib_pose.png}
  \caption{カメラキャリブレーション用の姿勢}
  \label{fig:camcalib_pose}
 \end{center}
\end{figure}

\subsection{カメラ取り付け位置のキャリブレーション}

次に，カメラの取り付け位置を計算します．
キャリブレーションプログラムは，学習データとしてロボットの各姿勢における
チェッカーボードの姿勢を入力とします(\figref{fig:handeye_calib})．チェッ
カーボードの認識にはROSのcheckerboard\_detectionノードを使用し，tfとして
publishされたチェッカーボードの姿勢をpythonプログラムで受信します．具体
的な手順は，付録に記載します．
\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=0.8\linewidth]{figure/handeye_calib.png}
  \caption{手先を動かしたチェッカーボードの検出}
  \label{fig:handeye_calib}
 \end{center}
\end{figure}

\chapter{デモの実行}

\section{制御系モジュールの起動と接続}

\begin{lstlisting}
 $ cd iv_plan/scripts
 $ ./run-hironx-interface.sh
\end{lstlisting}
これにより，両手に対応するカメラ，認識モジュール，HiroNXInterface制御モ
ジュールが起動，接続され，さらに各モジュールがactivateされます．
HiroNXInterfaceモジュールは起動後GUI上の「RTC Status」が緑になった状態で
「Set up Robot」ボタンを押し，RTCまわりの初期化を行う必要がある点に注意し
てください．起動したGUIから以下を実行しておきます．
\begin{enumerate}
 \item ジョイントキャリブレーション
 \item 初期姿勢へ移行
 \item 指のサーボオン　(ついでに開閉動作確認)
\end{enumerate}

\section{認識系モジュールの起動と接続}

次に，HiroNX両手の手先カメラのキャプチャ及び，AppRecog認識モジュールを起
動します．左右で同じ名前のコンポーネント群を起動するため，別々のRTC名前
空間に起動します．
\begin{lstlisting}
 $ ./run.sh
\end{lstlisting}
ここで，左右のカメラが逆になっていないかどうか確認してください．
逆になっている場合は，CaptureCameraLeft.confと
CaptureCameraRight.confに記述してあるconf.default.int\_camera\_idの番号を
逆にして，run.shを再実行してください．
左右それぞれのモジュール群は起動した端末でCtrl+cを押すことで終了します．

iv\_plan/scriptsにあるスクリプトは，ローカルのファイルシステム非依存な方
法でモジュールの場所を検出するため，ROSのコマンドであるrospackを使います．
rospackを利用できない場合は，適宜，スクリプトを編集するか同等の機能を実
現するコマンドに置き換えてください．
もし，コンポーネントが上手く接続およびactivateされない場合は，
./comcon.shおよび./comact.shを再度実行してみてください．

カメラが認識できていない可能性がある場合には，xawtvコマンドなどでカメラ自
体が認識されているか確認してください．
認識関係のモジュールを登録するネームサーバはscriptsディレクトリの
rtc\_left.confおよびrtc\_right.confで指定します．

\section{動作生成系モジュールの起動とモジュール間通信の確認}

以下のコマンドでデモプログラムを起動し，
\begin{itemize}
 \item HiroNX本体のモジュールと通信し，関節角度の取得および，動作コマンド
       の送信を行なうことができること
 \item 認識器と通信し認識結果を取得できること
\end{itemize}
を確認します.
\begin{lstlisting}[label=src:branch]
 $ cd iv_scenario/src
 $ ipython demo_wexpo.py

 >>> rr.get_joint_angles() # 実機の関節角度を取得
 >>> r.prepare()           # シミュレータのモデルの姿勢を変更
 >>> sync()                # 実機をモデルにあわせる(実機が動くので注意)

 図???のように，認識できる位置に対象物を置いて
 >>> rr.detect(sensor='rhandcam') # 右手ハンドカメラでの認識
 >>> rr.detect(sensor='lhandcam') # 左手ハンドカメラでの認識

 認識した後，その結果を世界座標への変換する
 >>> detect(sensor='rhandcam')
 >>> detect(sensor='lhandcam')

 >>> f = detect(sensor='rhandcam')
 >>> show_frame(f) # 認識結果の表示
 # シミュレータ内で認識位置に箱A0を移動させる
 >>> env.get_object('A0').locate(f) 
\end{lstlisting}
モジュールと通信できない場合は，iv\_scenario/srcにある
rtc.confで正しくネームサーバが指定されているか，ま
た，iv\_scenario/interface\_wexpo.pyに書かれている各モジュールのパスが正
しいかどうか確認してください．

\section{デモの実行}

\figref{fig:four_parts_on_the_table}のように作業台に対象物を4つ並べ，
 作業台上で両手を動かし，対象物を検出できるかどうかを確認してみます．

\begin{lstlisting}
 >>> look_for()
\end{lstlisting}
 上手く検出できれば，シミュレータ内の箱4つが正しい位置に移動します
 (\figref{fig:look_for}(a))．
 (b)では1つ，(c)では2つ認識に失敗しています．
 上手く検出されないときは，画像中で対象物が正しく認識されているかチェッ
 クしてください．

 検出時の初期位置へロボットの姿勢を持っていきたいときには,
以下のコマンドを実行します．
\begin{lstlisting}
 >>> go_scan_pose()
\end{lstlisting}

\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=0.45\linewidth]{figure/four_parts_on_the_table.jpg}
  \caption{４つの部品を並べた状態}
  \label{fig:four_parts_on_the_table}
 \end{center}
\end{figure}
\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=1.0\linewidth]{figure/look_for.png}
  \caption{作業台上の対象物認識結果}
  \label{fig:look_for}
 \end{center}
\end{figure}

 これで準備が整ったので，実際にデモを実行します．
 \begin{lstlisting}
  >>> demo()
 \end{lstlisting}
 作業台をスキャンし，対象物を4つ検出した後，
 最初は両手で，残りの２つは適宜持ち替えを行なって箱に配置します．
 対象物を4つ検出できなかったときは，動作には入りません．

 % 外部モジュールとの通信
 % >>> rr = MyHiroNxSystem(portdefs) # 外部モジュールとの通信インタフェー
 % スオブジェクトを作成


\newpage

\appendix
\chapter{キャリブレーションメモ}

checkerboard\_detectionおよび，camera\_calibrationパッケージを用意してく
ださい．ROSノード起動用のlaunchファイルセットのパッケージSense.tgzを使います．

\section{カメラキャリブレーションの手順}

\begin{verbatim}
$ roslaunch Sense rhand.launch
$ rosrun camera_calibration cameracalibrator.py --size 7x10 --square
		0.025 image:=/rhand/usb_cam/image_raw
Sense/launch/rhand.launchの内部パラメータを更新する
$ roslaunch Sense lhand.launch
$ rosrun camera_calibration cameracalibrator.py --size 7x10 --square
		0.025 image:=/lhand/usb_cam/image_raw
Sense/launch/lhand.launchの内部パラメータを更新する
\end{verbatim}

\section{カメラ取り付け位置キャリブレーションの手順}

\begin{enumerate}
 \item ROSのカメラノードにより画像および上記カメラパラメータがトピックと
       してpublishされ，checkerboard detectionノードでそれらをsubscribe
       し，推定したtfがpublishされるようにします． \\
       \verb|$ roslaunch Sense rhand\_checkerboard.launch|
 \item 机の上にチェッカーボードを置きます
 \item 手を動かして学習データを取ります．
       \verb|$ cd iv_scenario/src|\\
       \verb|$ ipython|\\
       \verb|>>> from hironx_calib import *|\\
       \verb|>>> rr.connect()|\\
       \verb|>>> res = record_data(sensor='rhandcam')|\\
       すべての姿勢で，チェッカーボードが視野に収まり，
       安定して認識できていることを確認します(\figref{fig:handcalib_checkerboard})．
 \item 手首リンクからカメラへの座標変換を計算します． \\
       \verb|>>> f = calibrate(res, link='RARM_JOINT5_Link', tf0=r.Trh_cam)|
 \item 結果を確認します． \\
       \verb|>>> play_data(res, link='RARM_JOINT5_Link', tf=f)|
 \item 問題なければhironx\_params.pyのTrh\_camを更新します．
 \item 左手カメラについても同様に行います．
\end{enumerate}

\begin{figure}[tbh]
 \begin{center}
  \includegraphics[width=0.6\linewidth]{figure/handcalib_checkerboard.png}
  \caption{checkerboard detector}
  \label{fig:handcalib_checkerboard}
 \end{center}
\end{figure}


% \addcontentsline{toc}{chapter}{参考文献}
% \markboth{参考文献}{参考文献}
% \bibliographystyle{junsrt}
% \bibliography{p2009}

\end{document}
